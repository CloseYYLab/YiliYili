## Deep Residual Learning for Image Recognition（用于图像识别的深度残差学习）
https://arxiv.org/abs/1512.03385

### Abstract
&emsp;深度神经网络训练更加困难。我们提出了一个残差学习框架，用于缓解比以前更深的网络的训练难度。**我们明确地将层重新表述为与层输入相关的学习残差函数，而不是学习无关的函数。我们提供了全面的实证证据表明这些残差网络更容易优化，并且可以从显著增加的深度中获得准确性提升**。在ImageNet数据集上，我们评估具有高达152层深度的残差网络，比VGG网络[41]深8倍，但复杂度仍然较低。这些残差网络的集合在ImageNet测试集上实现了3.57％的误差。这个结果赢得了ILSVRC 2015分类任务的第一名。我们还对具有100和1000层的CIFAR-10进行了分析。  


&emsp;对于许多视觉识别任务来说，表示的深度至关重要。仅仅由于我们极其深的表示方式，在COCO对象检测数据集上取得了28％的相对改进。深度残差网络是我们提交到ILSVRC＆COCO 2015竞赛的基础，我们还在ImageNet检测，ImageNet本地化，COCO检测和COCO分割等任务上获得了第一名。  

### Introduction
&emsp;深度卷积神经网络[22, 21]为图像分类[21, 50, 40]带来了一系列突破。深层网络自然地以[端到端](https://github.com/CloseYYLab/YiliYili/blob/main/%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9.md#%E7%AB%AF%E5%88%B0%E7%AB%AF)的多层方式整合了低/中/高级特征[50]和分类器，并且特征的“层级”可以通过堆叠的层数（深度）来增强。最近的证据[41, 44]表明，网络的深度至关重要，在具有挑战性的ImageNet数据集[36]上，所有领先的结果[41, 44, 13, 16]都利用了“非常深”的[41]模型，深度为十六[41]到三十[16]层。许多其他复杂的视觉识别任务[8, 12, 7, 32, 27]也极大地受益于非常深的模型。  


&emsp;由于深度的重要性，一个问题出现了：学习更好的网络是否就像简单地堆叠更多的层那样容易？回答这个问题的障碍是臭名昭著的[梯度消失/爆炸](https://github.com/CloseYYLab/YiliYili/blob/main/%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9.md#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8)问题[1, 9]，它从一开始就阻碍了收敛。然而，通过归一化初始化[23, 9, 37, 13]和中间归一化层[16]，这个问题在很大程度上得到了解决，它使得具有数十层的网络可以开始收敛于带有反向传播的随机梯度下降（SGD）[22]。  


&emsp;当更深的网络能够开始收敛时，出现了一个退化问题：随着网络深度的增加，准确性达到饱和（这可能是不足为奇的），然后迅速下降。令人意外的是，这种退化不是由过拟合引起的，在[11, 42]中报告，并经过我们的实验证实，将更多的层添加到适当深度的模型中会导致更高的训练误差。图1展示了一个典型的例子。  
![image](https://github.com/CloseYYLab/YiliYili/assets/56760687/66a27cdc-4a94-4eea-9e6b-4492e5312e6d)


&emsp;Figure 1.左图是在CIFAR-10数据集上使用20层和56层“plain”网络的训练误差，右图是测试误差。较深的网络具有较高的训练误差，因此也具有较高的测试误差。类似的现象在图4中展示了在ImageNet数据集上的情况。  


&emsp;训练准确率的下降表明，并非所有系统都容易优化。我们考虑一个较浅的架构和加入更多层的较深架构。存在这样一种解决方案：通过构造，深层模型中添加的层是恒等映射，其他层则从学习得到的较浅模型中复制而来。这种构造解的存在表明，深层模型的训练误差不应高于其相对较浅的对应模型。但实验结果表明，目前手头可用的求解器无法找到与构造解同等或更好的解（或无法在合理的时间内找到解）。  


&emsp;本文通过引入深度残差学习框架来解决退化问题。我们不再期望每几个堆叠的层能够直接拟合所需的底层映射，而是明确地让这些层拟合后面的残差映射。具体地，将所需的底层映射表示为 $H(x)$，我们让堆叠的非线性层拟合另一个映射 $F(x)：= H(x)- x$ 。原始映射被重新表示为 $F(x)+ x$ 。我们假设，优化残差映射比优化原始未参考的映射更容易。极端地，如果恒等映射是最优的，则将残差推向零比通过堆叠非线性层拟合恒等映射更容易。  
&emsp;**（此处为译者著：用通俗的话来讲，作者疑惑的点在于，即使一个网络什么都没学到，那么他只要输出原始的答案就可以，但是随着网络深度的加深，网络的学习能力反而发生退化。作者认为这是由于网络的学习过程并不是一个恒等映射，导致退化现象的产生。因此，作者提出了残差学习来解决退化现象。具体而言，假设输入为 $x$ 的时候，网络学习到的特征为 $H(x)$，我们想让网络最起码可以学习到特征 $x$ 来对抗退化现象，那么我们从先有学习到的特征 $H(x)$ 中剥离出固有特征 $x$ ，得到残差 $F(x) = H(x) - x$，让网络只学习残差 $F(x)$ ，这样可以保证网络不会发生退化现象）**


&emsp;公式 F(x) + x 可以通过使用“shortcut connections”（图2）的前馈神经网络来实现。“Shortcut connections”（2、34、49）是指跳过一个或多个层的连接。在我们的情况下，这些“shortcut connections”仅执行恒等映射，它们的输出被加到堆叠层的输出中（图2）。恒等“shortcut connections”既不增加额外的参数也不增加计算复杂度。整个网络仍然可以通过采用 SGD 反向传播的端对端训练，并且可以使用常见的库（例如 Caffe [19]）轻松实现，而无需修改求解器。  


&emsp;我们在ImageNet [36]上进行了全面的实验，以展示退化问题并评估我们的方法。我们展示了以下结果：1）我们极深的残差网络易于优化，而对应的“普通”网络（只是简单堆叠层）在深度增加时训练误差更高；2）我们的深度残差网络可以从大幅增加的深度中轻松获得准确性提升，产生比之前的网络更好的结果。  


&emsp;类似的现象也在CIFAR-10数据集[20]上得到了展示，这表明优化困难和我们方法的效果不仅适用于特定的数据集。我们成功地在该数据集上训练了超过100层的模型，并探索了超过1000层的模型。  


&emsp;在ImageNet分类数据集[36]上，我们通过极深的残差网络获得了出色的结果。我们的152层残差网络是迄今为止在ImageNet上展示的最深网络，但其复杂性仍低于VGG网络[41]。我们的集成模型在ImageNet测试集上的top-5误差率为3.57%，并在ILSVRC 2015分类竞赛中获得了第一名。这种极深表示在其他识别任务上也具有出色的泛化性能，并使我们在ILSVRC＆COCO 2015竞赛中进一步获得了ImageNet检测、ImageNet定位、COCO检测和COCO分割等方面的第一名。这些强有力的证据表明残差学习原理是通用的，并且我们希望它在其他视觉和非视觉问题中也适用。  

### Related Work
&emsp;**残差表示** 在图像识别中，VLAD（Vector of Locally Aggregated Descriptors）是一种通过与字典相关的残差向量进行编码的表示方法，而Fisher Vector可以被看作是VLAD的概率版本。这两种方法都是用于图像检索和分类的强大的浅层表示方法。对于向量量化而言，编码残差向量比编码原始向量更有效。  


&emsp;在低级视觉和计算机图形学中，为了解决偏微分方程（PDE），广泛使用的多重网格方法将系统重新表述为多个尺度下的子问题，其中每个子问题负责处理较粗和较细尺度之间的残差解。另一种替代多重网格的方法是分层基准预处理，它依赖于表示两个尺度之间残差向量的变量。研究发现，这些求解器比那些不了解解的残差性质的标准求解器收敛速度更快。这些方法表明，良好的重新表述或预处理可以简化优化过程。  


&emsp;**快捷连接**有关快捷连接的实践和理论已经研究了很长时间。早期训练多层感知器（MLP）的实践是从网络输入到输出添加一个线性层 [34, 49]。在[44, 24]中，一些中间层直接连接到辅助分类器，以解决梯度消失/爆炸的问题。[39, 38, 31, 47]的论文提出了通过快捷连接实现层响应、梯度和传播误差的居中方法。在[44]中，“Inception”层由一个快捷分支和几个更深的分支组成。与我们的工作同时进行的“Highway Networks” [42, 43]使用门控函数 [15] 实现了快捷连接。这些门是数据相关的，并且具有参数，与我们的无参数标识快捷连接不同。当门控快捷连接“关闭”（接近零）时，高速公路网络中的层表示非残差函数。相反，我们的表述始终学习残差函数；我们的标识快捷连接永远不会关闭，并且所有信息都始终通过，还会学习额外的残差函数。此外，高速公路网络在极大增加深度（例如超过100层）时并没有展示出准确性提升。  

### Deep Residual Learning
####  Residual Learning
&emsp;让我们将H(x)视为一个待适应的底层映射，由几个堆叠的层（不一定是整个网络）组成，其中x表示这些层的第一个输入。如果假设多个非线性层可以渐近地逼近复杂的函数，那么可以等同地假设它们可以渐近地逼近残差函数，即H(x) - x（假设输入和输出具有相同的维度）。因此，与其期望堆叠的层来逼近H(x)，我们明确地让这些层逼近一个残差函数F(x) := H(x) - x。原始函数因此变为F(x) + x。虽然两种形式都能够渐近地逼近所需的函数（如假设的那样），但学习的难易程度可能会有所不同。   


&emsp;这种重新表述是由于关于退化问题的反直觉现象所引发的（图1，左图）。正如我们在介绍中讨论的那样，如果添加的层可以构造为恒等映射，那么更深的模型的训练误差不应大于其较浅的对应模型。退化问题表明，求解器可能在通过多个非线性层逼近恒等映射方面遇到困难。通过残差学习的重新表述，如果恒等映射是最优的，求解器可能会简单地将多个非线性层的权重驱动向零以逼近恒等映射。  


&emsp;在实际情况下，恒等映射很少是最优的，但我们的重新表述可以帮助预处理问题。如果最优函数与恒等映射相比更接近于恒等映射而不是零映射，求解器可以更容易地找到相对于恒等映射的扰动，而不是将其作为一个新函数来学习。我们通过实验证明（图7），学习得到的残差函数通常具有小的响应，表明恒等映射提供了合理的预处理方式。  

#### Identity Mapping by Shortcuts
&emsp;我们在每几个堆叠的层中采用残差学习。如图2所示，我们定义一个建筑块。形式上，在本文中，我们将建筑块定义为：  


![image](https://github.com/CloseYYLab/YiliYili/assets/56760687/1b0a6257-041f-4391-9e42-1d5be4d8fe9c)


&emsp;这里的x和y分别是所考虑的层的输入向量和输出向量。函数F(x, {Wi})代表需要学习的残差映射。对于图2中的例子，它有两层，其中 F = W2σ(W1x)，其中 σ代表ReLU [29]，为简化符号，省略了偏置项。F + x操作是通过快捷连接和逐元素相加来完成的。我们在加法后采用第二个非线性（即σ(y)，见图2）。在公式（1）中的快捷连接既不会引入额外的参数，也不会增加计算复杂度。这不仅在实践中具有吸引力，而且在纯网络和残差网络之间的比较中也很重要。我们可以公平地比较同时具有相同参数数量、深度、宽度和计算成本（除了微不足道的逐元素加法）的纯/残差网络。在公式（1）中，x和F的维度必须相等。如果不是这种情况（例如，当改变输入/输出通道时），我们可以通过快捷连接进行线性投影Ws来匹配维度:  


![image](https://github.com/CloseYYLab/YiliYili/assets/56760687/4390300a-948b-4139-999c-46a47234556e)


&emsp;在公式（1）中，我们也可以使用一个方阵Ws，但通过实验证明，恒等映射已经足够解决退化问题并且更具经济性，因此只有在需要匹配维度时才使用Ws。残差函数F的形式是灵活的。本文的实验涉及到具有两层或三层的函数F（图5），但更多层次也是可能的。但是，如果F只有一个单层，那么公式（1）类似于一个线性层：y = W1x + x，并没有观察到优势。同时需要注意的是，虽然上述符号是关于全连接层的简洁表示，但它们同样适用于卷积层。函数F(x, {Wi})可以表示多个卷积层。逐元素的加法是逐通道对两个特征图执行的操作。  

