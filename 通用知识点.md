## 端到端  
端到端，是相对于 非端到端 而言的。在机器学习最开始的时候，人们并不是直接输入原始数据，获得最终结果；而是首先通过特征提取，对原始数据进行初步的处理，然后再对于得到的特征进行学习，得出分类or回归的结果。比如使用一些hand-crafted functions作为特征描述符等等。  
因此，在这种情况下（非端到端），特征的提取会对模型的最终表现有着巨大的影响。而特征描述符的书写又具有很大的经验成分，所以是一件比较困难的任务。  
**端到端的学习，就是把特征提取的任务也交给模型去做，直接输入原始数据或者经过些微预处理的数据，让模型自己进行特征提取。** 这一种设计风格在神经网络，尤其是深层的神经网络出现后开始被广泛应用，无疑是得益于算力的提升。并且，神经网络可以很好地学习到特征的描述，之前需要人工设计的特征算子，本身也可以通过神经网络的方式，让模型自己习得，从而可以用于其他的任务以及更深的研究。  

## 梯度消失/梯度爆炸
二者出现的原因相同：都是因为在梯度的求解需要涉及到链式求导法则，假设 $f(x) = g(x)h(x)y(x)j(x)k(x)$ 那么在链式求导后的过程中，$f'(x)$会存在一个参数： 
$`f'(x) = g'(x)h'(x)y'(x)j'(x)k'(x)`$  
当 $g'(x)，h'(x)，y'(x)，j'(x)，k'(x)$ 均大于1时，由于是累乘的关系，$f'(x)$的值会迅速增长到无穷大，这就是梯度爆炸  
当 $g'(x)，h'(x)，y'(x)，j'(x)，k'(x)$ 均小于1时，由于是累乘的关系，$f'(x)$的值会迅速衰减到无穷小，这就是梯度衰减  

### 梯度消失解决办法：  
1. 更换激活函数：使用 relu 函数来代替 sigmoid 函数可以缓解梯度消失问题。sigmoid 函数的导数只有在一段区间内不为0，其值过大或过小都会导致对应的导数变为0，因此替换为 relu 激活函数会缓解该问题，因为 relu 激活函数在 > 0 的部分导数恒等于1
   而引入 relu 函数可能带来的坏处是"死亡 relu"现象：由于负数部分恒为0，会导致一些神经元无法激活。对此的解决办法是使用 leaky relu 函数
2. 残差结构：由于残差结构会直接加上原来的梯度函数(假设为x)，在求导过程中，梯度部分(x)会变为1.而其余部分的累加和为-1的概率很少，这使得梯度不会接近0，有效的缓解了梯度消失

### 梯度爆炸解决办法：  
1. 梯度裁剪：其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。
2. 正则化：使用正则化对参数进行限制，当参数过大时会进行惩罚，以此来缓解梯度爆炸

### 通用解决办法：
BatchNormalization:BN层会将输出强行拉到以0为均值，以1为方差的分布。这样做的好处是将输出进行规范化，从而避免因输出值过大或过小所带来的梯度爆炸/梯度消失现象（激活函数导数为0的区域为饱和区，该区域会导致出现梯度消失问题。 BN 层会将梯度规范到激活函数的非饱和区）  
为了弥补变换时带来的损失，BN 又添加了两个而外的可学习参数 β 和 γ 。  

参考链接：https://www.cnblogs.com/PythonLearner/p/13173560.html
