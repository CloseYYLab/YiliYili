## 端到端  
端到端，是相对于 非端到端 而言的。在机器学习最开始的时候，人们并不是直接输入原始数据，获得最终结果；而是首先通过特征提取，对原始数据进行初步的处理，然后再对于得到的特征进行学习，得出分类or回归的结果。比如使用一些hand-crafted functions作为特征描述符等等。  
因此，在这种情况下（非端到端），特征的提取会对模型的最终表现有着巨大的影响。而特征描述符的书写又具有很大的经验成分，所以是一件比较困难的任务。  
**端到端的学习，就是把特征提取的任务也交给模型去做，直接输入原始数据或者经过些微预处理的数据，让模型自己进行特征提取。** 这一种设计风格在神经网络，尤其是深层的神经网络出现后开始被广泛应用，无疑是得益于算力的提升。并且，神经网络可以很好地学习到特征的描述，之前需要人工设计的特征算子，本身也可以通过神经网络的方式，让模型自己习得，从而可以用于其他的任务以及更深的研究。  

## 梯度消失/梯度爆炸
二者出现的原因相同：都是因为在梯度的求解需要涉及到链式求导法则，假设 $f(x) = g(x)h(x)y(x)j(x)k(x)$ 那么在链式求导后的过程中，f'(x)会存在一个参数： 
$`f'(x) = g'(x)h'(x)y'(x)j'(x)k'(x)`$`
